{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled89.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Initialise libraries"
      ],
      "metadata": {
        "id": "EWCFcxk5JETG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download Pytorch Geometric\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "id": "mC_dvBCHdRMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the libraries\n",
        "import os\n",
        "import torch\n",
        "import torch_geometric.datasets as datasets\n",
        "import torch_geometric.data as data\n",
        "import torch_geometric.transforms as transforms\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from itertools import combinations, groupby\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GCNConv, GATConv\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import median_absolute_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "from collections import namedtuple\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, Flatten\n",
        "\n",
        "import sklearn\n",
        "import torch.nn.utils.prune as prune\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "sCLg23JyhbWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation"
      ],
      "metadata": {
        "id": "Iol_MstdEoqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set seed \n",
        "np.random.seed(1234)\n",
        "\n",
        "#Generate a random NetworkX graph\n",
        "def gnp_random_connected_graph(n, p):\n",
        "    \"\"\"\n",
        "    Generates a random undirected graph, similarly to an Erdős-Rényi \n",
        "    graph, but enforcing that the resulting graph is conneted\n",
        "    \"\"\"\n",
        "    edges = combinations(range(n), 2)\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(range(n))\n",
        "    if p <= 0:\n",
        "        return G\n",
        "    if p >= 1:\n",
        "        return nx.complete_graph(n, create_using=G)\n",
        "    for _, node_edges in groupby(edges, key=lambda x: x[0]):\n",
        "        node_edges = list(node_edges)\n",
        "        random_edge = random.choice(node_edges)\n",
        "        G.add_edge(*random_edge)\n",
        "        for e in node_edges:\n",
        "            if random.random() < p:\n",
        "                G.add_edge(*e)\n",
        "    return G\n",
        "\n",
        "    #Reference [1]: https://stackoverflow.com/questions/61958360/how-to-create-random-graph-where-each-node-has-at-least-1-edge-using-networkx"
      ],
      "metadata": {
        "id": "uClRU_UJLRw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZHYDQHgLNxY"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1234)\n",
        "\n",
        "#Create 200 graphs with 5 nodes\n",
        "for i in range(1, 200):\n",
        "  j = 0\n",
        "  # j = j/10\n",
        "  k = random.randint(1,42)\n",
        "  globals()['G_'+str(i)] = gnp_random_connected_graph(5, j) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define node attributes\n",
        "#Randomise x and y coordinates for each of the 5 nodes\n",
        "np.random.seed(1234)\n",
        "\n",
        "\n",
        "for i in range(1,200):\n",
        "\n",
        "  x_cord_0 =  random.randint(50,60)\n",
        "  x_cord_1 = random.randint(45,50)\n",
        "  x_cord_2 = random.randint(40,50)\n",
        "  x_cord_3 = random.randint(10,20)\n",
        "  x_cord_4 = random.randint(10,20)\n",
        "  \n",
        "  y_cord_0 = random.randint(60,70)\n",
        "  y_cord_1 = random.randint(40,50)\n",
        "  y_cord_2 = random.randint(50,60)\n",
        "  y_cord_3 = random.randint(60,70)\n",
        "  y_cord_4 = random.randint(45,50)\n",
        "\n",
        "#Set node attributes\n",
        "  pos_x={0:x_cord_0, 1:x_cord_1, 2 :x_cord_2, 3 :x_cord_3, 4:x_cord_4}\n",
        "  pos_y= {0:y_cord_0, 1:y_cord_1, 2 :y_cord_2, 3 :y_cord_3, 4:y_cord_4}\n",
        "  nx.set_node_attributes(globals()['G_'+str(i)], pos_x, 'pos_x')\n",
        "  nx.set_node_attributes(globals()['G_'+str(i)], pos_y, 'pos_y')"
      ],
      "metadata": {
        "id": "Yyd_ItECRMWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualise a graph\n",
        "def viz(G):\n",
        "  fig, ax = plt.subplots(figsize = (15,8))\n",
        "  initialpos = {0:(nx.get_node_attributes(G, \"pos_x\")[0], nx.get_node_attributes(G, \"pos_y\")[0]), 1:(nx.get_node_attributes(G, \"pos_x\")[1], nx.get_node_attributes(G, \"pos_y\")[1]), \n",
        "                2:(nx.get_node_attributes(G, \"pos_x\")[2], nx.get_node_attributes(G, \"pos_y\")[2]), 3:(nx.get_node_attributes(G, \"pos_x\")[3], nx.get_node_attributes(G, \"pos_y\")[3]), \n",
        "                4:(nx.get_node_attributes(G, \"pos_x\")[4], nx.get_node_attributes(G, \"pos_y\")[4])}\n",
        "  nx.draw(G, with_labels = True, pos = initialpos)\n",
        "  limits=plt.axis('on') # turns on axis\n",
        "  ax.set_xlim(0,120)\n",
        "  ax.set_ylim(0,120)\n",
        "  ax.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)"
      ],
      "metadata": {
        "id": "XOnrUPFIsufW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualise graph\n",
        "viz(G_1)"
      ],
      "metadata": {
        "id": "wqrCr3d_t1uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MAST formulation"
      ],
      "metadata": {
        "id": "b5JL6fLwEvQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding connections based on the smallest distance(area) between nodes\n",
        "list_1 = [0,1,2,3,4]\n",
        "list_2 = [0,1,2,3,4]\n",
        "\n",
        "def graph_dataset(G): \n",
        "  edge_1 = []\n",
        "  edge_2 = []\n",
        "  distance = []\n",
        "  for u in list_1:\n",
        "    for v in list_2:\n",
        "      df = pd.DataFrame(columns = ['edge_1','edge_2', 'distance'])\n",
        "      if(u!=v):\n",
        "        dist = round((((nx.get_node_attributes(G, 'pos_x')[u]  - nx.get_node_attributes(G, 'pos_x')[v])**2 + (nx.get_node_attributes(G, 'pos_y')[u] - nx.get_node_attributes(G, 'pos_y')[v])**2)**0.5),2)\n",
        "        # x = random.randint(0, 5)\n",
        "        # dist = x + dist\n",
        "        edge_1.append(u)\n",
        "        edge_2.append(v)\n",
        "        distance.append(dist)\n",
        "        \n",
        "  df['edge_1'] = edge_1\n",
        "  df['edge_2'] = edge_2\n",
        "  df['distance'] = distance\n",
        "  df.drop(df[ (df['edge_1'] == 0) & (df['edge_2'] == 2)].index, inplace = True)\n",
        "  df.drop(df[ (df['edge_1'] == 0) & (df['edge_2'] == 3)].index, inplace = True)\n",
        "  df.drop(df[ (df['edge_1'] == 0) & (df['edge_2'] == 4)].index, inplace = True)\n",
        "  indexes = df[ (df['edge_1'] == 1) & (df['edge_2'] == 0)].index\n",
        "  df.drop(indexes, inplace = True)\n",
        "  indexes = df[ (df['edge_1'] == 1) & (df['edge_2'] == 3)].index\n",
        "  df.drop(indexes, inplace = True)\n",
        "  indexes = df[ (df['edge_1'] == 1) & (df['edge_2'] == 4)].index\n",
        "  df.drop(indexes, inplace = True)\n",
        "  indexes = df[ (df['edge_1'] == 2) & (df['edge_2'] == 0)].index\n",
        "  df.drop(indexes, inplace = True)\n",
        "  # indexes = df[ (df['edge_1'] == 2) & (df['edge_2'] == 3) ].index\n",
        "  # df.drop(indexes, inplace = True)\n",
        "  # df.drop(df[ (df['edge_1'] == 2) & (df['edge_2'] == 4)].index, inplace = True)\n",
        "  df.drop(df[ (df['edge_1'] == 2) & (df['edge_2'] == 1)].index, inplace = True)\n",
        "\n",
        "  df.sort_values(by = ['distance'], inplace = True)\n",
        "\n",
        "  min_df = pd.DataFrame() \n",
        "  for i in df.edge_1.unique():\n",
        "    globals()['df_'+str(i)] = df[df['edge_1'] == i]\n",
        "    globals()['df_'+str(i)] = globals()['df_'+str(i)][globals()['df_'+str(i)].distance == globals()['df_'+str(i)].distance.min()]\n",
        "    min_df = min_df.append(globals()['df_'+str(i)], ignore_index = True)\n",
        "  \n",
        "  return min_df\n"
      ],
      "metadata": {
        "id": "nUrx1mFGVG4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Iterate the MAST function over the 200 graphs\n",
        "for i in range(1,200):\n",
        "  globals()['min_df_'+str(i)] = graph_dataset(globals()['G_'+str(i)])\n",
        "  globals()['min_df_'+str(i)].sort_values( by = ['edge_1'], inplace = True)\n",
        "  globals()['min_df_'+str(i)].reset_index (inplace = True)\n",
        "  globals()['min_df_'+str(i)].drop(columns = ['index'], inplace = True)\n",
        " \n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "G3tEzI_oTFws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Further MAST formulation\n",
        "#Ensuring that the constraint is met, i.e. node x and node y can only be connected if and only if, coverage area of node x & coverage area of node y > distance between x and y\n",
        "for i in range(1, 200):\n",
        "    if((globals()['min_df_'+str(i)].iloc[0,0] == 0) & (globals()['min_df_'+str(i)].iloc[0,1] == 1)):\n",
        "      if(globals()['min_df_'+str(i)].iloc[0,2]>globals()['min_df_'+str(i)].iloc[1,2]): #0,1\n",
        "        globals()['min_df_'+str(i)].iloc[1,2] = globals()['min_df_'+str(i)].iloc[0,2]\n",
        "      else:\n",
        "        globals()['min_df_'+str(i)].iloc[0,2] = globals()['min_df_'+str(i)].iloc[1,2]\n",
        "    if((globals()['min_df_'+str(i)].iloc[2,0] == 2) & (globals()['min_df_'+str(i)].iloc[2,1] == 3)): #2,3\n",
        "      if(globals()['min_df_'+str(i)].iloc[2,2]>globals()['min_df_'+str(i)].iloc[3,2]):\n",
        "        globals()['min_df_'+str(i)].iloc[3,2] = globals()['min_df_'+str(i)].iloc[2,2]\n",
        "      else:\n",
        "        globals()['min_df_'+str(i)].iloc[2,2] = globals()['min_df_'+str(i)].iloc[3,2]\n",
        "    if((globals()['min_df_'+str(i)].iloc[2,0] == 2) & (globals()['min_df_'+str(i)].iloc[2,1] == 4)): #2,4\n",
        "      if(globals()['min_df_'+str(i)].iloc[2,2]>globals()['min_df_'+str(i)].iloc[4,2]):\n",
        "        globals()['min_df_'+str(i)].iloc[4,2] = globals()['min_df_'+str(i)].iloc[2,2]\n",
        "      else:\n",
        "        globals()['min_df_'+str(i)].iloc[2,2] =globals()['min_df_'+str(i)].iloc[4,2]\n",
        "  \n",
        "    \n",
        "  \n",
        " \n"
      ],
      "metadata": {
        "id": "YF53zfq_QWHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set coverage area as a node attribute for the nodes\n",
        "for i in range(1,200):\n",
        "  pow={}\n",
        "  pow={0:globals()['min_df_'+str(i)]['distance'][0], 1:globals()['min_df_'+str(i)]['distance'][1], 2:globals()['min_df_'+str(i)]['distance'][2], 3:globals()['min_df_'+str(i)]['distance'][3], 4:globals()['min_df_'+str(i)]['distance'][4]}\n",
        "  nx.set_node_attributes(globals()['G_'+str(i)], pow, 'y')\n",
        "  "
      ],
      "metadata": {
        "id": "qeMiwsb8Ry4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding edges where the constraint is met, i.e. edge exist between a pair of nodes if and only if their individual coverage area is greater than the distance between the pair of nodes\n",
        "for i in range(1,200):\n",
        "  globals()['G_'+str(i)].add_edge(0, 1)\n",
        "  cov_0 = nx.get_node_attributes(globals()['G_'+str(i)], 'y')[0]\n",
        "  cov_1 = nx.get_node_attributes(globals()['G_'+str(i)], 'y')[1]\n",
        "  cov_2 = nx.get_node_attributes(globals()['G_'+str(i)], 'y')[2]\n",
        "  cov_3 = nx.get_node_attributes(globals()['G_'+str(i)], 'y')[3]\n",
        "  cov_4 = nx.get_node_attributes(globals()['G_'+str(i)], 'y')[4]\n",
        "  dist_12 = round((((nx.get_node_attributes(globals()['G_'+str(i)], 'pos_x')[1]  - nx.get_node_attributes(globals()['G_'+str(i)], 'pos_x')[2])**2 + (nx.get_node_attributes(globals()['G_'+str(i)], 'pos_y')[1] - nx.get_node_attributes(globals()['G_'+str(i)], 'pos_y')[2])**2)**0.5),2)\n",
        "  dist_23 = round((((nx.get_node_attributes(globals()['G_'+str(i)], 'pos_x')[2]  - nx.get_node_attributes(globals()['G_'+str(i)], 'pos_x')[3])**2 + (nx.get_node_attributes(globals()['G_'+str(i)], 'pos_y')[2] - nx.get_node_attributes(globals()['G_'+str(i)], 'pos_y')[3])**2)**0.5),2)\n",
        "  dist_24 = round((((nx.get_node_attributes(globals()['G_'+str(i)], 'pos_x')[2]  - nx.get_node_attributes(globals()['G_'+str(i)], 'pos_x')[4])**2 + (nx.get_node_attributes(globals()['G_'+str(i)], 'pos_y')[2] - nx.get_node_attributes(globals()['G_'+str(i)], 'pos_y')[4])**2)**0.5),2)\n",
        "  dist_34 = round((((nx.get_node_attributes(globals()['G_'+str(i)], 'pos_x')[3]  - nx.get_node_attributes(globals()['G_'+str(i)], 'pos_x')[4])**2 + (nx.get_node_attributes(globals()['G_'+str(i)], 'pos_y')[3] - nx.get_node_attributes(globals()['G_'+str(i)], 'pos_y')[4])**2)**0.5),2)\n",
        "  if((cov_1 >= dist_12) & (cov_2 >= dist_12)):\n",
        "    globals()['G_'+str(i)].add_edge(1, 2)\n",
        "  if((cov_2>=dist_23) & (cov_3>=dist_23)):\n",
        "    globals()['G_'+str(i)].add_edge(2, 3)  \n",
        "  if((cov_2>=dist_24) & (cov_4>=dist_24)):\n",
        "    globals()['G_'+str(i)].add_edge(2, 4)  \n",
        "  if((cov_3>=dist_34) & (cov_4>=dist_34)):\n",
        "    globals()['G_'+str(i)].add_edge(3, 4)  "
      ],
      "metadata": {
        "id": "0qOD9gT5UJwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Converting NetworkX graph to Pytorch Geometric Objects"
      ],
      "metadata": {
        "id": "qv5tDt1LHgNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Appeding all the graphs to an empty list\n",
        "Graphs = []\n",
        "for i in range(1, 200):\n",
        "  Graphs.append(globals()['G_'+str(i)])"
      ],
      "metadata": {
        "id": "bcrQaCPcEJDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert to PyG objects with features as node coordinates\n",
        "for i in range(1,199):\n",
        "  globals()['data_'+str(i)] = from_networkx(Graphs[i],['pos_x','pos_y'])"
      ],
      "metadata": {
        "id": "QW5IVA9JkWXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#List of data for dataloader\n",
        "data_list = [data_1,\tdata_2,\tdata_3,\tdata_4,\tdata_5,\tdata_6,\tdata_7,\tdata_8,\tdata_9,\tdata_10,\tdata_11,\tdata_12,\tdata_13,\tdata_14,\tdata_15,\tdata_16,\tdata_17,\tdata_18,\tdata_19,\tdata_20,\tdata_21,\tdata_22,\tdata_23,\tdata_24,\tdata_25,\tdata_26,\tdata_27,\tdata_28,\tdata_29,\tdata_30,\tdata_31,\tdata_32,\tdata_33,\tdata_34,\tdata_35,\tdata_36,\tdata_37,\tdata_38,\tdata_39,\tdata_40,\tdata_41,\tdata_42,\tdata_43,\tdata_44,\tdata_45,\tdata_46,\tdata_47,\tdata_48,\tdata_49,\tdata_50,\tdata_51,\tdata_52,\tdata_53,\tdata_54,\tdata_55,\tdata_56,\tdata_57,\tdata_58,\tdata_59,\tdata_60,\tdata_61,\tdata_62,\tdata_63,\tdata_64,\tdata_65,\tdata_66,\tdata_67,\tdata_68,\tdata_69,\tdata_70,\tdata_71,\tdata_72,\tdata_73,\tdata_74,\tdata_75,\tdata_76,\tdata_77,\tdata_78,\tdata_79,\tdata_80,\tdata_81,\tdata_82,\tdata_83,\tdata_84,\tdata_85,\tdata_86,\tdata_87,\tdata_88,\tdata_89,\tdata_90,\tdata_91,\tdata_92,\tdata_93,\tdata_94,\tdata_95,\tdata_96,\tdata_97,\tdata_98,\tdata_99,\tdata_100,\tdata_101,\tdata_102,\tdata_103,\tdata_104,\tdata_105,\tdata_106,\tdata_107,\tdata_108,\tdata_109,\tdata_110,\tdata_111,\tdata_112,\tdata_113,\tdata_114,\tdata_115,\tdata_116,\tdata_117,\tdata_118,\tdata_119,\tdata_120,\tdata_121,\tdata_122,\tdata_123,\tdata_124,\tdata_125,\tdata_126,\tdata_127,\tdata_128,\tdata_129,\tdata_130,\tdata_131,\tdata_132,\tdata_133,\tdata_134,\tdata_135,\tdata_136,\tdata_137,\tdata_138,\tdata_139,\tdata_140,\tdata_141,\tdata_142,\tdata_143,\tdata_144,\tdata_145,\tdata_146,\tdata_147,\tdata_148,\tdata_149,\tdata_150,\tdata_151,\tdata_152,\tdata_153,\tdata_154,\tdata_155,\tdata_156,\tdata_157,\tdata_158,\tdata_159,\tdata_160,\tdata_161,\tdata_162,\tdata_163,\tdata_164,\tdata_165,\tdata_166,\tdata_167,\tdata_168,\tdata_169,\tdata_170,\tdata_171,\tdata_172,\tdata_173,\tdata_174,\tdata_175,\tdata_176,\tdata_177,\tdata_178,\tdata_179,\tdata_180,\tdata_181,\tdata_182,\tdata_183,\tdata_184,\tdata_185,\tdata_186,\tdata_187,\tdata_188,\tdata_189,\tdata_190,\tdata_191,\tdata_192,\tdata_193,\tdata_194,\tdata_195,\tdata_196,\tdata_197,\tdata_198]"
      ],
      "metadata": {
        "id": "_o9vZWEarkPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split data into train and test\n",
        "train_data = data_list[0:190]\n",
        "\n",
        "test_data = data_list[190:]"
      ],
      "metadata": {
        "id": "8ucv6M2jpDfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare data before feeding into our GCN \n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n"
      ],
      "metadata": {
        "id": "c7bMNKhtkW1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GCN"
      ],
      "metadata": {
        "id": "cj5N1bJm21CP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define model"
      ],
      "metadata": {
        "id": "QxHPCFyPFF5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GCN Architecture\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        \n",
        "        self.conv1 = GATConv(2, 256)\n",
        "        #self.conv2 = GATConv(256, 128)\n",
        "        # self.conv3 = GCNConv(128, 56)\n",
        "        # self.conv4 = GCNConv(56, 28)\n",
        "        #self.conv3 = GCNConv(data.num_features, 32)\n",
        "        \n",
        "        #self.linear1 = torch.nn.Linear(128,64)\n",
        "        self.conv3 = GCNConv(256,128)\n",
        "        self.linear1 = torch.nn.Linear(128,64)\n",
        "        self.linear2 = torch.nn.Linear(64,1)\n",
        "        #self.linear3 = torch.nn.Linear(32,1)\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        #x = self.conv2(x, edge_index)\n",
        "        #x = x.relu()\n",
        "        #x = torch.flatten(x)\n",
        "        # x = self.conv2(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.conv3(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.conv4(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.conv2(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.linear1(x)\n",
        "        # x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.linear1(x)\n",
        "        #x= x.relu()\n",
        "        x = self.linear2(x)\n",
        "        # x = self.linear3(x)\n",
        "        #x= x.relu()\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "gHQGKDMZlYqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialise our GCN model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN(256).to(device)"
      ],
      "metadata": {
        "id": "NuLyKWxi00Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define optimiser\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "nrXA4svp1uyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model architecture\n",
        "print(model)"
      ],
      "metadata": {
        "id": "O4DYs-DeLE6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train our model"
      ],
      "metadata": {
        "id": "ReqqJOxsFIwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train our model and evaluate on the test set\n",
        "train_pred = []\n",
        "train_values = []\n",
        "val_pred = []\n",
        "val_values = []\n",
        "for e in range(1000):\n",
        "    print(e)\n",
        "    model.train()     # Optional when not using Model Specific layer\n",
        "    for data in train_loader:\n",
        "\n",
        "      optim.zero_grad()\n",
        "      data.x = data.x.to(device, dtype = torch.float)\n",
        "      data.y = data.y.to(device, dtype = torch.float)\n",
        "\n",
        "      pred = model(data.x, data.edge_index)\n",
        "      \n",
        "      loss = F.mse_loss(pred.squeeze(), data.y.squeeze())\n",
        "      pred_np = pred.detach().numpy()\n",
        "      data.y_np = data.y.detach().numpy() \n",
        "      train_pred.append(pred_np)\n",
        "      train_values.append(data.y_np)\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "    \n",
        "    model.eval()\n",
        "    for data in train_loader:\n",
        "           # Optional when not using Model Specific layer\n",
        "      optim.zero_grad()\n",
        "      data.x = data.x.to(device, dtype = torch.float)\n",
        "      data.y = data.y.to(device, dtype = torch.float)\n",
        "      \n",
        "      pred = model(data.x, data.edge_index)\n",
        "      \n",
        "      loss = F.mse_loss(pred.squeeze(), data.y.squeeze())\n",
        "      pred_np = pred.detach().numpy()\n",
        "      data.y_np = data.y.detach().numpy() \n",
        "      val_pred.append(pred_np)\n",
        "      val_values.append(data.y_np)\n",
        "        "
      ],
      "metadata": {
        "id": "WViLYFG_8Teo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualise learning curves"
      ],
      "metadata": {
        "id": "Xlb71wX7FCQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtain the training/validation curves\n",
        "def train_graph(pred, train, val_pred, val_train, epochs):\n",
        "\n",
        "  median_scores_train = []\n",
        "  median_scores_test = []\n",
        "\n",
        "  mean_scores_train = []\n",
        "  mean_scores_test = []\n",
        "\n",
        "  for i in range(1,epochs):\n",
        "    median_scores_train.append(median_absolute_error(pred[i], train[i]))\n",
        "    mean_scores_train.append(mean_absolute_error(pred[i], train[i]))\n",
        "    median_scores_test.append(median_absolute_error(val_pred[i], val_train[i]))\n",
        "    mean_scores_test.append(mean_absolute_error(val_pred[i], val_train[i]))\n",
        "\n",
        "  epoch = []\n",
        "  for i in range(1,epochs):\n",
        "    epoch.append(i)\n",
        "\n",
        "  ysmoothed_median_train = gaussian_filter1d(median_scores_train, sigma=2)\n",
        "  ysmoothed_mean_train = gaussian_filter1d(mean_scores_train, sigma=2)\n",
        "\n",
        "  ysmoothed_median_test = gaussian_filter1d(median_scores_test, sigma=2)\n",
        "  ysmoothed_mean_test = gaussian_filter1d(mean_scores_test, sigma=2)\n",
        "\n",
        "  plt.rcParams['figure.figsize'] = [7, 7]\n",
        "  plt.plot(epoch, ysmoothed_median_train, label = 'Train Median Absolute error')\n",
        "  plt.plot(epoch, ysmoothed_mean_train, label = 'Train Mean Absolute error')\n",
        "\n",
        "  plt.plot(epoch, ysmoothed_median_test, label = 'Test Median Absolute error')\n",
        "  plt.plot(epoch, ysmoothed_mean_test, label = 'Test Mean Absolute error')\n",
        "\n",
        "  listOf_Xticks = np.arange(0, epochs, epochs/10)\n",
        "  plt.xticks(listOf_Xticks)\n",
        "\n",
        "  plt.ylim([0, 12])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Error\")\n",
        "  plt.title(\"Median/Mean absolute error vs Epochs(GCN)\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  print(\"Mimimum test median absolute error\", min(median_scores_test))\n",
        "  print(\"Mimimum test mean absolute error\", min(mean_scores_test))"
      ],
      "metadata": {
        "id": "ZP4-OfnstoZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Learning curves for the GCN\n",
        "train_graph(train_pred, train_values, val_pred, val_values, 1000)"
      ],
      "metadata": {
        "id": "tWqoOkn7uw0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data preparation(CNN and MLP)"
      ],
      "metadata": {
        "id": "Kz-qbE64Cype"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating our dataset\n",
        "for i in range(1,50):\n",
        "  globals()['min_df_'+str(i)] = graph_dataset(globals()['G_'+str(i)])\n",
        "  globals()['min_df_'+str(i)]\n",
        "  for index, row in globals()['min_df_'+str(i)].iterrows():\n",
        "    globals()['G_'+str(i)].add_edge(row['edge_1'], row['edge_2'])\n",
        "\n",
        "  globals()['min_df_'+str(i)].sort_values( by = ['edge_1'], inplace = True) \n",
        "  globals()['min_df_'+str(i)]['distance'] = globals()['min_df_'+str(i)]['distance'].astype(int)\n",
        "  pow={0:globals()['min_df_'+str(i)].iloc[0, 2], 1:globals()['min_df_'+str(i)].iloc[1, 2], 2:globals()['min_df_'+str(i)].iloc[2, 2], 3:globals()['min_df_'+str(i)].iloc[3, 2], 4:globals()['min_df_'+str(i)].iloc[4, 2]}\n",
        "  nx.set_node_attributes(globals()['G_'+str(i)], pow, 'y')\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "2zX_5GgyQo6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation\n",
        "Graph = []\n",
        "coord_x = []\n",
        "coord_y = []\n",
        "coord_near_x = []\n",
        "coord_near_y = []\n",
        "distance = []\n",
        "for i in range(1, 200):\n",
        "    for index, row in globals()['min_df_'+str(i)].iterrows():\n",
        "      Graph.append(i)\n",
        "      coord_x.append(nx.get_node_attributes(globals()['G_'+str(i)], 'pos_x')[row['edge_1']])\n",
        "      \n",
        "      coord_y.append(nx.get_node_attributes(globals()['G_'+str(i)], 'pos_y')[row['edge_1']])\n",
        "\n",
        "      distance.append(row['distance'])"
      ],
      "metadata": {
        "id": "UWJVrvgSQygn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assigning values in the dataset\n",
        "combined_data = pd.DataFrame(columns = ['Graph', 'coord_x', 'coord_y', 'distance'])\n",
        "combined_data['Graph'] = Graph\n",
        "combined_data['coord_x'] = coord_x\n",
        "combined_data['coord_y'] = coord_y\n",
        "combined_data['distance'] = distance\n"
      ],
      "metadata": {
        "id": "9y5PwNZxTbd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Look at the first few rows of our dataset\n",
        "combined_data"
      ],
      "metadata": {
        "id": "TZWrvVElY3wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MLP"
      ],
      "metadata": {
        "id": "S1LWD75nFXup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train/Test split"
      ],
      "metadata": {
        "id": "23xVy3wGDPhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting our training and testing data\n",
        "x = combined_data[['coord_x', 'coord_y']]\n",
        "y = combined_data[['distance']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.10, random_state=42, shuffle=False)"
      ],
      "metadata": {
        "id": "jS5MjSTSXoKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train model"
      ],
      "metadata": {
        "id": "EtS0u_dfFkKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialise our MLP regressor\n",
        "clf = MLPRegressor(random_state=20, max_iter=300).fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "3hS6z66kYvo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluate for the test set"
      ],
      "metadata": {
        "id": "XaWPIRJpEU0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict on test set\n",
        "pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "KnVd8--ilpMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean absolute error for the MLP\n",
        "mean_absolute_error(y_test, pred)"
      ],
      "metadata": {
        "id": "r-VJYpbhl3mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Median absolute error for the MLP\n",
        "median_absolute_error(y_test, pred)"
      ],
      "metadata": {
        "id": "KcJX-ajvlc6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convolution Neural Network"
      ],
      "metadata": {
        "id": "NTTu-CHpD7cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare data for our CNN"
      ],
      "metadata": {
        "id": "7SvlWLolDvr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshape values for CNN\n",
        "x = x.values\n",
        "x = x.reshape(x.shape[0], x.shape[1], 1)"
      ],
      "metadata": {
        "id": "REedzAcNmD6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train/Test split"
      ],
      "metadata": {
        "id": "xw3QiJ9_F2nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state=42, shuffle=False)"
      ],
      "metadata": {
        "id": "ISUn-1a-mGk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define our model"
      ],
      "metadata": {
        "id": "E_iIlX1pF6hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define our CNN \n",
        "model = Sequential()\n",
        "model.add(Conv1D(32, 2, activation=\"relu\", input_shape=(2, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss=[\"mae\"], optimizer=\"adam\")"
      ],
      "metadata": {
        "id": "Yts27nQcmNKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "LNjfNSXNNhIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train CNN"
      ],
      "metadata": {
        "id": "NMzZAVV-F8tT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train model\n",
        "history = model.fit(X_train, y_train, batch_size=32,epochs=300, verbose=1, validation_data = (X_test,y_test))"
      ],
      "metadata": {
        "id": "cEfW3M2qmQpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluate for the test set"
      ],
      "metadata": {
        "id": "X6B0pEN0EYwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict for Test set\n",
        "ypred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "kejZ8WeMmUEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Median absolute error for the CNN\n",
        "median_absolute_error(y_test, ypred)"
      ],
      "metadata": {
        "id": "pga569npmqyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nean absolute error for the CNN\n",
        "mean_absolute_error(y_test, ypred)"
      ],
      "metadata": {
        "id": "ypF7qIsFmtK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Federated learning with GCN"
      ],
      "metadata": {
        "id": "b4cPOtLG1Dzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare dataset for each client"
      ],
      "metadata": {
        "id": "on-saKIrE1az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create multiple train_loader for different clients\n",
        "for i in range(0, 6):\n",
        "\n",
        "  ran = random.randint(0,175)\n",
        "  globals()['train_data_'+str(i)] = data_list[ran:ran+25]\n"
      ],
      "metadata": {
        "id": "1e-Nl80JZe8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,6):\n",
        "\n",
        "  globals()['train_loader_'+str(i)] = DataLoader((globals()['train_data_'+str(i)]), batch_size=10, shuffle=False)"
      ],
      "metadata": {
        "id": "g0st3NYrZbKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initialise FL parameters"
      ],
      "metadata": {
        "id": "dBe7o0PqGG6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FL parameters\n",
        "num_clients = 10\n",
        "num_selected = 6\n",
        "num_rounds = 150\n",
        "epochs = 50\n",
        "batch_size = 10"
      ],
      "metadata": {
        "id": "1_WdRwvU1OWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initialise Global model"
      ],
      "metadata": {
        "id": "kqQPRZ7PGe06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GCN Architecture\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        \n",
        "        self.conv1 = GATConv(2, 256)\n",
        "        #self.conv2 = GATConv(256, 128)\n",
        "        # self.conv3 = GCNConv(128, 56)\n",
        "        # self.conv4 = GCNConv(56, 28)\n",
        "        #self.conv3 = GCNConv(data.num_features, 32)\n",
        "        \n",
        "        #self.linear1 = torch.nn.Linear(128,64)\n",
        "        self.conv3 = GCNConv(256,128)\n",
        "        self.linear1 = torch.nn.Linear(128,64)\n",
        "        self.linear2 = torch.nn.Linear(64,1)\n",
        "        #self.linear3 = torch.nn.Linear(32,1)\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        #x = self.conv2(x, edge_index)\n",
        "        #x = x.relu()\n",
        "        #x = torch.flatten(x)\n",
        "        # x = self.conv2(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.conv3(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.conv4(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.conv2(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.linear1(x)\n",
        "        # x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.linear1(x)\n",
        "        #x= x.relu()\n",
        "        x = self.linear2(x)\n",
        "        # x = self.linear3(x)\n",
        "        #x= x.relu()\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "EqbBo35v1V2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train model"
      ],
      "metadata": {
        "id": "l7KRk4brGlNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train client model on client data\n",
        "def client_update(client_model, optimizer, train_loader, epoch):\n",
        "    \"\"\"\n",
        "    This function updates/trains client model on client data\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for e in range(epoch):\n",
        "        for data in train_loader:\n",
        "          total_loss = 0\n",
        "          opt.zero_grad()\n",
        "          data.x = data.x.to(device, dtype = torch.float)\n",
        "          data.y = data.y.to(device, dtype = torch.float)\n",
        "          pred = model(data.x, data.edge_index)\n",
        "          loss = F.mse_loss(pred.squeeze(), data.y.squeeze())\n",
        "          #total_loss += loss.item() * data.num_graphs\n",
        "    \n",
        "          loss.backward()\n",
        "          opt.step()\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "4PaCuj1o13vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fedaveraging\n",
        "def server_aggregate(global_model, client_models):\n",
        "    \"\"\"\n",
        "    This function has aggregation method 'mean'\n",
        "    \"\"\"\n",
        "    ### This will take simple mean of the weights of models ###\n",
        "    global_dict = global_model.state_dict()\n",
        "    for k in global_dict.keys():\n",
        "      global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))], 0).mean(0)\n",
        "      global_model.load_state_dict(global_dict)\n",
        "    for model in client_models:\n",
        "      model.load_state_dict(global_model.state_dict())"
      ],
      "metadata": {
        "id": "91I9AvVO4LvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define global model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "global_model = GCN(256).to(device)"
      ],
      "metadata": {
        "id": "wpKgCQpP1-0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define client model as an object of the global model\n",
        "client_models = [ global_model for _ in range(num_selected)]"
      ],
      "metadata": {
        "id": "ussNjEzn5Af6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialise weight parameters\n",
        "for model in client_models:\n",
        "    model.load_state_dict(global_model.state_dict())"
      ],
      "metadata": {
        "id": "2f-n9oAv5Rka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define optimiser\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "hO3nX_QZ5o7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Federated training with 6 clients and 150 rounds for each epoch\n",
        "for r in range(num_rounds):\n",
        "    print(\"Round\", r)\n",
        "    # select random clients\n",
        "    client_idx = np.random.permutation(num_clients)[:num_selected]\n",
        "    loss = 0\n",
        "    for i in tqdm(range(num_selected)):\n",
        "        loss += client_update(client_models[i], torch.optim.Adam(model.parameters(), lr=0.001), globals()['train_data_'+str(i)], epoch=5)"
      ],
      "metadata": {
        "id": "fSHUSaDM698F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Aggregate client state for global model\n",
        "server_aggregate(global_model, client_models)"
      ],
      "metadata": {
        "id": "zvC5squ26_Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluate for GCN with federated averaging"
      ],
      "metadata": {
        "id": "y_qBqmA3HHd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict for test set using the global model\n",
        "pred_ = []\n",
        "test_values = []\n",
        "for data in test_loader:\n",
        "      data.x = data.x.to(device, dtype = torch.float)\n",
        "      #data.y = data.y.to(device, dtype = torch.float)\n",
        "      pred = global_model(data.x, data.edge_index)\n",
        "\n",
        "      loss = F.mse_loss(pred.squeeze(), data.y.squeeze())\n",
        "      pred = pred.detach().numpy()\n",
        "      data.y = data.y.detach().numpy() \n",
        "      \n",
        "      pred_.append(pred)\n",
        "      test_values.append(data.y)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R99tAi1TKTGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Conversion\n",
        "pred_ = np.asarray(pred_)\n",
        "test_values = np.asarray(test_values)"
      ],
      "metadata": {
        "id": "b4EmCRGzssH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Median absolute error \n",
        "mae = median_absolute_error(np.ndarray.flatten(pred_), np.ndarray.flatten(test_values))\n",
        "print(mae)"
      ],
      "metadata": {
        "id": "SAoeJ1iZKq3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean absolute error \n",
        "pred_ = np.asarray(pred_)\n",
        "test_values = np.asarray(test_values)\n",
        "\n",
        "mean_absolute_error(np.ndarray.flatten(pred_), np.ndarray.flatten(test_values))\n"
      ],
      "metadata": {
        "id": "4tHzduEyKubN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pruning"
      ],
      "metadata": {
        "id": "BLhHgE3A99qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.conv1.lin_src.weight"
      ],
      "metadata": {
        "id": "W3N_AeK9L2E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialise model"
      ],
      "metadata": {
        "id": "MRwnoz1vJMJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GCN Architecture\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        \n",
        "        self.conv1 = GATConv(2, 256)\n",
        "        #self.conv2 = GATConv(256, 128)\n",
        "        # self.conv3 = GCNConv(128, 56)\n",
        "        # self.conv4 = GCNConv(56, 28)\n",
        "        #self.conv3 = GCNConv(data.num_features, 32)\n",
        "        \n",
        "        #self.linear1 = torch.nn.Linear(128,64)\n",
        "        self.conv3 = GCNConv(256,128)\n",
        "        self.linear1 = torch.nn.Linear(128,64)\n",
        "        self.linear2 = torch.nn.Linear(64,1)\n",
        "        #self.linear3 = torch.nn.Linear(32,1)\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        #x = self.conv2(x, edge_index)\n",
        "        #x = x.relu()\n",
        "        #x = torch.flatten(x)\n",
        "        # x = self.conv2(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.conv3(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.conv4(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.conv2(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.linear1(x)\n",
        "        # x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.linear1(x)\n",
        "        #x= x.relu()\n",
        "        x = self.linear2(x)\n",
        "        # x = self.linear3(x)\n",
        "        #x= x.relu()\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "pVqptEtSRqTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN(256).to(device)"
      ],
      "metadata": {
        "id": "z8QGwcOd-I0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define optimiser\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "jsndEfVP-Len"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train initial network"
      ],
      "metadata": {
        "id": "vF-8IBlDJecc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train model\n",
        "for epoch in range(1, 300):\n",
        "  print(epoch)\n",
        "  for data in train_loader:\n",
        "    total_loss = 0\n",
        "    optim.zero_grad()\n",
        "    data.x = data.x.to(device, dtype = torch.float)\n",
        "    data.y = data.y.to(device, dtype = torch.float)\n",
        "    pred = model(data.x, data.edge_index)\n",
        "    loss = F.mse_loss(pred.squeeze(), data.y.squeeze())\n",
        "    #total_loss += loss.item() * data.num_graphs\n",
        "    \n",
        "    loss.backward()\n",
        "    optim.step()"
      ],
      "metadata": {
        "id": "IZTxaAxp-OPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate on test set"
      ],
      "metadata": {
        "id": "7aZAD-GIJjOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict for test set\n",
        "pred_ = []\n",
        "test_values = []\n",
        "for data in test_loader:\n",
        "    data.x = data.x.to(device, dtype = torch.float)\n",
        "    #data.y = data.y.to(device, dtype = torch.float)\n",
        "    pred = model(data.x, data.edge_index)\n",
        "\n",
        "    loss = F.mse_loss(pred.squeeze(), data.y.squeeze())\n",
        "    pred = pred.detach().numpy()\n",
        "    data.y = data.y.detach().numpy() \n",
        "    pred_.append(pred)\n",
        "    test_values.append(data.y)\n"
      ],
      "metadata": {
        "id": "-oZdvzlr-RVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Median absolute error \n",
        "pred_ = np.asarray(pred_)\n",
        "test_values = np.asarray(test_values)\n",
        "\n",
        "median_absolute_error(np.ndarray.flatten(pred_), np.ndarray.flatten(test_values))\n"
      ],
      "metadata": {
        "id": "QGXM39_X-UuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean absolute error\n",
        "pred_ = np.asarray(pred_)\n",
        "test_values = np.asarray(test_values)\n",
        "\n",
        "mean_absolute_error(np.ndarray.flatten(pred_), np.ndarray.flatten(test_values))\n"
      ],
      "metadata": {
        "id": "w-TNDAXmBkWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model weights\n",
        "torch.save(model.state_dict(), 'model_weights_.pth')"
      ],
      "metadata": {
        "id": "wDoZ2t1eBSnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Magnitude-based pruning based on lowest L1 score"
      ],
      "metadata": {
        "id": "ZZpNdVjYJ65v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pruning \n",
        "def prune_(proportion, GCN, layer1, layer2, layer3):\n",
        "  parameters_to_prune = (\n",
        "      (GCN.layer1, 'weight'),\n",
        "      (GCN.layer2, 'weight'),\n",
        "      (GCN.layer3, 'weight')\n",
        "  )\n",
        "\n",
        "  prune.global_unstructured(\n",
        "      parameters_to_prune,\n",
        "      pruning_method=prune.L1Unstructured,\n",
        "      amount=proportion,\n",
        "  )"
      ],
      "metadata": {
        "id": "9UNBEUS2BgSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load model weights\n",
        "model.load_state_dict(torch.load('model_weights_.pth'))"
      ],
      "metadata": {
        "id": "8jRl0pn6Pl3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply pruning\n",
        "prune_(0.1, model, conv1.lin_src, conv3.lin, linear1)"
      ],
      "metadata": {
        "id": "CBf1uDt4PdIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Re-train"
      ],
      "metadata": {
        "id": "EC22HRHQQdfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Re-Train model\n",
        "for epoch in range(1, 300):\n",
        "  print(epoch)\n",
        "  for data in train_loader:\n",
        "    total_loss = 0\n",
        "    optim.zero_grad()\n",
        "    data.x = data.x.to(device, dtype = torch.float)\n",
        "    data.y = data.y.to(device, dtype = torch.float)\n",
        "    pred = model(data.x, data.edge_index)\n",
        "    loss = F.mse_loss(pred.squeeze(), data.y.squeeze())\n",
        "    #total_loss += loss.item() * data.num_graphs\n",
        "    \n",
        "    loss.backward()\n",
        "    optim.step()"
      ],
      "metadata": {
        "id": "AgVG-eXyQfJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluate on test set"
      ],
      "metadata": {
        "id": "vwyvBGe1QiB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict for test set\n",
        "pred_ = []\n",
        "test_values = []\n",
        "for data in test_loader:\n",
        "    data.x = data.x.to(device, dtype = torch.float)\n",
        "    #data.y = data.y.to(device, dtype = torch.float)\n",
        "    pred = model(data.x, data.edge_index)\n",
        "\n",
        "    loss = F.mse_loss(pred.squeeze(), data.y.squeeze())\n",
        "    pred = pred.detach().numpy()\n",
        "    data.y = data.y.detach().numpy() \n",
        "    pred_.append(pred)\n",
        "    test_values.append(data.y)\n"
      ],
      "metadata": {
        "id": "XdqVQvDtQmqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Median absolute error \n",
        "pred_ = np.asarray(pred_)\n",
        "test_values = np.asarray(test_values)\n",
        "\n",
        "median_absolute_error(np.ndarray.flatten(pred_), np.ndarray.flatten(test_values))\n"
      ],
      "metadata": {
        "id": "kQa8THY6QrgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean absolute error\n",
        "pred_ = np.asarray(pred_)\n",
        "test_values = np.asarray(test_values)\n",
        "\n",
        "mean_absolute_error(np.ndarray.flatten(pred_), np.ndarray.flatten(test_values))\n"
      ],
      "metadata": {
        "id": "qOy36fp6Qt3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Magnitude-based pruning based on threshold"
      ],
      "metadata": {
        "id": "W47USN_wQ0Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialise model\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(2, 256)\n",
        "        self.conv2 = GCNConv(256, 128)\n",
        "        #self.conv3 = GCNConv(128, 1)\n",
        "        #self.conv3 = GCNConv(data.num_features, 32)\n",
        "        self.linear1 = torch.nn.Linear(128,1)\n",
        "        # self.linear2 = torch.nn.Linear(56,1)\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        # x = self.conv3(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.conv2(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        # x = self.conv2(x, edge_index)\n",
        "        # x = x.relu()\n",
        "        x = self.linear1(x)\n",
        "        # x = x.relu()\n",
        "        # x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "pzMfV1VreIcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN(256).to(device)"
      ],
      "metadata": {
        "id": "XmuVZWINMtix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Threshold pruning\n",
        "class ThresholdPruning(prune.BasePruningMethod):\n",
        "    PRUNING_TYPE = \"unstructured\"\n",
        "\n",
        "    def __init__(self, threshold):\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def compute_mask(self, tensor, default_mask):\n",
        "        return torch.abs(tensor) > self.threshold"
      ],
      "metadata": {
        "id": "bUlp3GJ6Mt78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load previous weights\n",
        "model.load_state_dict(torch.load('model_weights_.pth'))"
      ],
      "metadata": {
        "id": "0aInHjDxPD0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mention parameters\n",
        "parameters_to_prune = (\n",
        "    (model.conv1.lin_src, 'weight'),\n",
        "    (model.conv3.lin, 'weight'),\n",
        "    (model.linear1, 'weight')\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune, pruning_method=ThresholdPruning, threshold=0.01\n",
        ")"
      ],
      "metadata": {
        "id": "P8GJLtNQPEa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Re-train"
      ],
      "metadata": {
        "id": "d6JrOfWeReUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 300):\n",
        "  print(epoch)\n",
        "  for data in train_loader:\n",
        "    total_loss = 0\n",
        "    optim.zero_grad()\n",
        "    data.x = data.x.to(device, dtype = torch.float)\n",
        "    data.y = data.y.to(device, dtype = torch.float)\n",
        "    pred = model(data.x, data.edge_index)\n",
        "    loss = F.mse_loss(pred.squeeze(), data.y.squeeze())\n",
        "    #total_loss += loss.item() * data.num_graphs\n",
        "    \n",
        "    loss.backward()\n",
        "    optim.step()"
      ],
      "metadata": {
        "id": "jWQ1uztMRkKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluate on test set"
      ],
      "metadata": {
        "id": "Kg3tDSSARnZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict for test set\n",
        "pred_ = []\n",
        "test_values = []\n",
        "for data in test_loader:\n",
        "    data.x = data.x.to(device, dtype = torch.float)\n",
        "    #data.y = data.y.to(device, dtype = torch.float)\n",
        "    pred = model(data.x, data.edge_index)\n",
        "\n",
        "    loss = F.mse_loss(pred.squeeze(), data.y.squeeze())\n",
        "    pred = pred.detach().numpy()\n",
        "    data.y = data.y.detach().numpy() \n",
        "    pred_.append(pred)\n",
        "    test_values.append(data.y)\n"
      ],
      "metadata": {
        "id": "xgtaQCv4TqCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Median absolute error \n",
        "pred_ = np.asarray(pred_)\n",
        "test_values = np.asarray(test_values)\n",
        "\n",
        "median_absolute_error(np.ndarray.flatten(pred_), np.ndarray.flatten(test_values))\n"
      ],
      "metadata": {
        "id": "nBQh1Qw4R0aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean absolute error\n",
        "pred_ = np.asarray(pred_)\n",
        "test_values = np.asarray(test_values)\n",
        "\n",
        "mean_absolute_error(np.ndarray.flatten(pred_), np.ndarray.flatten(test_values))\n"
      ],
      "metadata": {
        "id": "wkx8LVBgR2ck"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}